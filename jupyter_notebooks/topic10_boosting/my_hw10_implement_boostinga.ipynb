{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: программист-исследователь Mail.ru Group, старший преподаватель <br>Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 10\n",
    "## <center> Реализация градиентного бустинга\n",
    "\n",
    "В этом задании мы реализуем алгоритм градиентного бустинга в довольно общем виде, один и тот же класс будет описывать бинарный классификатор, при обучении которого минимизируется логистическая фунцкция потерь, и 2 регрессора, минимизирующих среднеквадратическую ошибку MSE и [RMSLE](https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError). Это даст представление о том, что с помощью градиентного бустинга можно оптимизировать произвольные дифференцируемые функции потерь, а также что бустинг адаптируется под разные задачи. [Веб-форма](https://goo.gl/forms/mMUhGSDiOHJI9NHN2) для ответов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Версию алгоритма берем из [статьи](https://habrahabr.ru/company/ods/blog/327250/#klassicheskiy-gbm-algoritm-friedman-a) (см. псевдокод), но с двумя упрощениями:\n",
    "- инициализация – средним значением вектора $\\large y$, то есть $\\large \\hat{f_0} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$\n",
    "- шаг градиентного спуска (то же что и вес очередного базового алгоритма в композиции) постоянный: $\\large \\rho_t = const$\n",
    "\n",
    "Соответствие обозначений в псевдокоде и в классе `GradientBoosting`, который мы сейчас напишем:\n",
    "- $\\large \\{x_i, y_i\\}_{i = 1,\\ldots n}$ или `X`, `y` – обучающая выборка\n",
    "- $\\large L(y,f)$ или `objective` – функция потерь\n",
    "- $\\large \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}$ или `objective_grad` – градиент функции потерь\n",
    "- $М$ или `n_estimators` – число итераций бустинга\n",
    "- $\\large h(x,\\theta)$ или `DecisionionTreeRegressor` – базовый алгоритм, дерево решений для регрессии\n",
    "- $\\large \\theta$ – гиперпараметры деревьев, мы рассмотрим только `max_depth` и `random_state`\n",
    "- $\\large \\rho_t$ или `learning_rate` – коэффициент, с которым  $\\large h_t(x,\\theta)$ входит в композицию, $t=1,\\ldots,M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление градиентов log_loss, MSE и RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала классика – возьмем ручку и бумажку и посчитаем градиенты функций потерь:\n",
    "\n",
    "$$log\\_loss(y, p) = -y\\log p - (1 - y)\\log (1 - p) = -\\sum_{i=1}^{n}y_i\\log p_i + (1 - y_i)\\log (1 - p_i)$$\n",
    "\n",
    "$$MSE(y, p) = \\frac{1}{n}(y - p)^T(y - p) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2$$\n",
    "\n",
    "$$RMSLE(y, p) = \\sqrt{\\frac{1}{n} (\\log (p + 1) - \\log (y + 1))^T(\\log (p + 1) - \\log (y + 1))} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(\\log (p_i + 1) - \\log (y_i + 1))^2}$$\n",
    "\n",
    "Здесь $y$ и $p$ – это **векторы** истинных ответов и прогнозов соответственно.\n",
    "`log_loss` взяли как в  `sklearn` – для случая меток целевого класса 0 и 1, а не -1 и 1, как описано в статье (только не усреднили, т.е. нет деления на число объектов $n$, в `sklearn` это соответствует значению аргумента `normalize=False` функции `log_loss`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 1.</font> Какова формула градиента функции `MSE`?\n",
    " 1. $(p - y) \\hspace{5cm}$ 3. $2(p - y)$\n",
    " <br><br>\n",
    " 2. $\\frac{2}{n}(y - p) \\hspace{4.7cm}$ 4. $\\frac{2}{n}(p - y)$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 2.</font> Какова формула градиента функции `log_loss`?\n",
    " 1. $\\large \\frac{y - p}{y(1 - y)} \\hspace{5.2cm}$ 3. $\\large \\frac{p - y}{p(1 - p)}$\n",
    " <br><br>\n",
    " 2. $\\large \\frac{y - p}{p(1 - p)}\\hspace{5.2cm}$ 4. $\\large \\frac{p - y}{y(1 - y)}$\n",
    " <br><br>\n",
    " *Примечание:* деление на вектор – покомпонентное, например $\\frac{1}{p} = (\\frac{1}{p_1}, \\ldots \\frac{1}{p_n})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Вопрос 3.</font> Какова формула градиента функции `RMSLE`?\n",
    " 1. $\\frac{1}{n} (p + 1) RMSLE^{-1}(y, p) \\log \\frac{p+1}{y+1} \\hspace{5cm}$ 3. $[n (y + 1) RMSLE(y, p)]^{-1} \\log \\frac{p+1}{y+1} $\n",
    " <br><br>\n",
    " 2. $[n (p + 1) RMSLE(y, p)]^{-1} \\log \\frac{p+1}{y+1} \\hspace{5cm}$ 4. $\\frac{1}{n} \\frac{y+1}{(p + 1)} RMSLE^{-1}(y, p) \\log \\frac{p+1}{y+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `GradientBoosting`. Детали:\n",
    "- класс наследуется от `sklearn.base.BaseEstimator`;\n",
    "- конструктор принимает параметры `loss` – название фунцкии потерь, которая будет оптимизироваться (`log_loss`, `mse` (по умолчанию) или `rmsle`), `n_estimators` – число деревьев (т.е. число итераций бустинга, по умолчанию – 10), `learning_rate` – шаг градиентного спуска (по умолчанию $10^{-2}$), `max_depth` – ограничение на максимальную глубину деревьев (по умолчанию 3) и `random_state` – сид генерации псевдослучайных чисел, нужен только для деревьев (по умолчанию 17);\n",
    "- в зависимости от переданного значения `loss` инициализируются `objective` и `objective_grad`. Для `MSE` берем `sklearn.metrics.mean_squared_error`, для `log_loss` – `sklearn.metrics.log_loss` (без нормализации, то есть на число объектов не надо делить, формула дана выше), а `RMSLE` и градиенты всех трех функций надо реализовать самостоятельно. При подсчете градиентов не выкидывайте константы типа двойки или $n$;\n",
    "- в реализациях градиентов `log_loss` и `rmsle` будет покомпонентное деление на вектора. Чтобы избежать деление на 0, предварительно замените значения, меньшие $10^{-5}$, на $10^{-5}$. Но только там, где необходимо. Например, в случае вычисления $\\frac{y}{p}$ замены делаем только в векторе $p$;\n",
    "- также в конструкторе создаются списки `loss_by_iter_` и `residuals_by_iter_` для отлаживания работы алгоритма и `trees_` – для хранения обученных деревьев;\n",
    "- класс имеет методы `fit`, `predict` и `predict_proba`\n",
    "- метод `fit` принимает матрицу `X` и вектор `y` (объекты `numpy.array`), а возвращает текущий экземпляр класса `GradientBoosting`, т.е. `self`. Основная логика, конечно же, реализуется здесь. На каждой итерации текущее значение функции потерь записывается в `loss_by_iter_`, значение антиградиента (то что в статье названо псевдо-остатками) – в `residuals_by_iter_` (можно в конструктор добавить флаг `debug=False` и добавлять значения антиградиента только при включенном флаге). Также обученное дерево добавляется в список `trees_`;\n",
    "- метод `predict_proba` возвращает линейную комбинацию прогнозов деревьев. Не забудем тут и про начальное приближение. В случае регрессии название метода будет не очень удачным, но оставим так, чтоб не писать отдельно классификатор и регрессор. В случае классификации к ответу применяется $\\sigma$-преобразование. В реализации $\\sigma$-функции замените значения аргумента, превышающие по модулю 100, на -100 или +100 в зависимости от знака (чтоб избежать underflow & overflow);\n",
    "- метод `predict` в случае регрессии возвращает линейную комбинацию прогнозов деревьев (+ начальное приближение), то есть то же, что и `predict_proba`. В случае классификации метод `predict` задействует метод `predict_proba` и возвращает вектор из 0 и 1, полученный сравнением предсказанных вероятностей с некоторым порогом, при котором максимизируется доля правильных ответов на обучающей выборке. Здесь хорошо было бы решить одномерную задачу оптимизации, но для полной воспроизводимости давайте выбирать порог из `np.linspace(0.01, 1.01, 100)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import load_breast_cancer, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "??log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoosting(BaseEstimator):\n",
    "    \n",
    "    def sigma(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def log_loss_grad(self, y, p):\n",
    "        denom = p * (1 - p)\n",
    "        denom = np.where(denom < 1e-5, 1e-5, denom)\n",
    "        return (p - y) / denom\n",
    "    \n",
    "    def mse_grad(self, y, p):\n",
    "        return 2 / y.shape[0] * (p - y)\n",
    "    \n",
    "    def rmsle(self, y, p):\n",
    "        return np.sqrt(1 / y.shape[0] * np.sum(np.square(np.log(p + 1) - np.log(y + 1))))\n",
    "    \n",
    "    def rmsle_grad(self, y, p):\n",
    "        denom = y.shape(0) * (p + 1) * self.rmsle(y, p)\n",
    "        denom = 1e-5 if denom < 1e-5 else denom\n",
    "        return 1 / denom * (np.log(p + 1) - np.log(y + 1))\n",
    "    \n",
    "    def __init__(self, loss='mse', n_estimators=10, learning_rate=1e-2, max_depth=3, random_state=17):\n",
    "\n",
    "        if loss == 'mse':\n",
    "            self.objective = mean_squared_error\n",
    "            self.objective_grad = self.mse_grad\n",
    "        elif loss == 'log_loss':\n",
    "            self.objective = log_loss\n",
    "            self.objective_grad = self.log_loss_rad\n",
    "        elif loss == 'rmsle': \n",
    "            self.objective = self.rmsle\n",
    "            self.objective_grad = self.rmsle_grad\n",
    "        else: \n",
    "            raise Exception('invalid loss function')\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.loss_by_iter_ = []\n",
    "        self.residuals_by_iter_ = [] \n",
    "        self.trees_ = []\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        На каждой итерации текущее значение функции потерь записывается в loss_by_iter_, \n",
    "        значение антиградиента (то что в статье названо псевдо-остатками) – в residuals_by_iter_ \n",
    "        (можно в конструктор добавить флаг debug=False и добавлять значения антиградиента только при включенном флаге). \n",
    "        Также обученное дерево добавляется в список trees_;\n",
    "        \n",
    "        f0 = np.mean(y)\n",
    "        for j in range(n_estimators):\n",
    "            \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Регрессия с игрушечным примером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_regr_toy = np.arange(7).reshape(-1, 1)\n",
    "y_regr_toy = ((X_regr_toy - 3) ** 2).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCDcAJEO4ASAZwg0AyRBuAEiGcANAMv8F4G3S5ZkuEz4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf0f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_regr_toy, y_regr_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессор `GradientBoosting` с функцией потерь `MSE` и параметрами `learning_rate`=0.1,  `max_depth`=3 – 200 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Можно также посмотреть на приближение и остатки на первых нескольких итерациях, как это делалось в статье."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессор `GradientBoosting` с теми же параметрами, но функцию потерь измените на `RMSLE`. Посмотрите на те же картинки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с игрушечным примером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_clf_toy = np.c_[np.arange(7), (np.arange(7) - 3) ** 2]\n",
    "y_clf_toy = np.array([0, 1, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d18cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_clf_toy[:, 0], X_clf_toy[:, 1], c=y_clf_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите классификатор `GradientBoosting` с функцией потерь `log_loss` и параметрами `learning_rate`=0.05,  `max_depth`=3 – 10 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Можно также посмотреть на приближение и остатки на первых нескольких итерациях, как это делалось в статье.\n",
    "\n",
    "\n",
    "\n",
    "<font color='red'>Вопрос 4.</font> Посчитайте предсказанные вероятности отнесения к классу +1 для всех 7 объектов игрушечной выборки. Каковы 2 уникальных значения в полученном векторе?\n",
    "1. 0.42 и 0.77\n",
    "2. 0.36 и 0.82\n",
    "3. 0.48 и 0.53\n",
    "4. 0.46 и 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия с UCI boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обучите регрессор `GradientBoosting` с функцией потерь `MSE` и параметрами `learning_rate`=3,  `max_depth`=10 – 300 итераций\n",
    "- Посмотрите на изменение функции потерь по итерациям бустинга\n",
    "- Сделайте прогнозы для отложенной выборки\n",
    "- Постройте распределение ответов `y_test` на отложенной выборке и наложите на него распределение ответов бустинга `test_pred`. Используйте метод `hist` из `matplotlib.pyplot` с параметром `bins=15`\n",
    "\n",
    "<font color='red'>Вопрос 5.</font> Выберите верное утверждение про гистограммы:\n",
    "1. Ответы бустинга в среднем завышены на 10 \n",
    "2. В бине, в который попадает медиана ответов на тестовой выборке (`numpy.median(y_test)`), больше значений из вектора прогнозов `test_pred`, чем из вектора ответов `y_test`\n",
    "3. Бустинг иногда прогнозирует значения, сильно выпадающие за диапазон изменения ответов `y_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с UCI breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите классификатор `GradientBoosting` с функцией потерь `log_loss` и параметрами `learning_rate`=0.01,  `max_depth`=3 – 200 итераций. Посмотрите на изменение функции потерь по итерациям бустинга. Сделайте прогнозы для отложенной выборки – как предсказанные вероятности отнесения к классу +1, так и бинарные прогнозы. Посчитайте ROC AUC для прогноза в виде вероятностей и долю правильных ответов для прогноза в виде меток классов.\n",
    "\n",
    "<font color='red'>Вопрос 6.</font> Каковы получаются ROC AUC и доля правильных ответов на отложенной выборке `(X_test, y_test)`?\n",
    "1. 0.99 и 0.97\n",
    "2. 1 и 0.97\n",
    "3. 0.98 и 0.96\n",
    "4. 0.97 и 0.95"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
