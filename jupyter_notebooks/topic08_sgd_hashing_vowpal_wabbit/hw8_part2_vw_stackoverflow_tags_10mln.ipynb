{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "</center>\n",
    "Автор материала: программист-исследователь Mail.ru Group, старший преподаватель <br>Факультета Компьютерных Наук ВШЭ Юрий Кашницкий. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 8. Часть 2\n",
    "## <center> Vowpal Wabbit в задаче классификации тегов вопросов на Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План 2 части домашнего задания\n",
    "    2.1. Введение\n",
    "    2.2. Описание данных\n",
    "    2.3. Предобработка данных\n",
    "    2.4. Обучение и проверка моделей\n",
    "    2.5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Введение\n",
    "\n",
    "В этом задании вы будете делать примерно то же, что я каждую неделю –  в Mail.ru Group: обучать модели на выборке в несколько гигабайт. Задание можно выполнить и на Windows с Python, но я рекомендую поработать под \\*NIX-системой (например, через Docker) и активно использовать язык bash.\n",
    "Немного снобизма (простите, но правда): если вы захотите работать в лучших компаниях мира в области ML, вам все равно понадобится опыт работы с bash под UNIX.\n",
    "\n",
    "[Веб-форма](https://goo.gl/forms/z8zENbMiaEAeB7nG3) для ответов.\n",
    "\n",
    "Для выполнения задания понадобится установленный Vowpal Wabbit (уже есть в докер-контейнере курса, см. инструкцию в README [репозитория](https://github.com/Yorko/mlcourse_open) нашего курса) и примерно 70 Гб дискового пространства. Я тестировал решение не на каком-то суперкомпе, а на Macbook Pro 2015 (8 ядер, 16 Гб памяти), и самая тяжеловесная модель обучалась около 12 минут, так что задание реально выполнить и с простым железом. Но если вы планируете когда-либо арендовать сервера Amazon, можно попробовать это сделать уже сейчас.\n",
    "\n",
    "Материалы в помощь:\n",
    " - интерактивный [тьюториал](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) CodeAcademy по утилитам командной строки UNIX (примерно на час-полтора)\n",
    " - [статья](https://habrahabr.ru/post/280562/) про то, как арендовать на Amazon машину (еще раз: это не обязательно для выполнения задания, но будет хорошим опытом, если вы это делаете впервые)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеются 10 Гб вопросов со StackOverflow – [скачайте](https://cloud.mail.ru/public/3bwi/bFYHDN5S5) и распакуйте архив. \n",
    "\n",
    "Формат данных простой:<br>\n",
    "<center>*текст вопроса* (слова через пробел) TAB *теги вопроса* (через пробел)\n",
    "\n",
    "Здесь TAB – это символ табуляции.\n",
    "Пример первой записи в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -1 ../../data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь у нас текст вопроса, затем табуляция и теги вопроса: *css, css3* и *css-selectors*. Всего в выборке таких вопросов 10 миллионов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!wc -l ../../data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, что такие данные я уже не хочу загружать в оперативную память и, пока можно, буду пользоваться эффективными утилитами UNIX –  head, tail, wc, cat, cut и прочими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте выберем в наших данных все вопросы с тегами *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift* и подготовим обучающую выборку в формате Vowpal Wabbit. Будем решать задачу 10-классовой классификации вопросов по перечисленным тегам.\n",
    "\n",
    "Вообще, как мы видим, у каждого вопроса может быть несколько тегов, но мы упростим себе задачу и будем у каждого вопроса выбирать один из перечисленных тегов либо игнорировать вопрос, если таковых тегов нет. \n",
    "Но вообще VW поддерживает multilabel classification (аргумент  --multilabel_oaa).\n",
    "<br>\n",
    "<br>\n",
    "Реализуйте в виде отдельного файла `preprocess.py` код для подготовки данных. Он должен отобрать строки, в которых есть перечисленные теги, и переписать их в отдельный файл в формат Vowpal Wabbit. Детали:\n",
    " - скрипт должен работать с аргументами командной строки: с путями к файлам на входе и на выходе\n",
    " - строки обрабатываются по одной (можно использовать tqdm для подсчета числа итераций)\n",
    " - если табуляций в строке нет или их больше одной, считаем строку поврежденной и пропускаем\n",
    " - в противном случае смотрим, сколько в строке тегов из списка *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift*. Если ровно один, то записываем строку в выходной файл в формате VW: `label | text`, где `label` – число от 1 до 10 (1 - *javascript*, ... 10 – *swift*). Пропускаем те строки, где интересующих тегов больше или меньше одного \n",
    " - из текста вопроса надо выкинуть двоеточия и вертикальные палки, если они есть – в VW это спецсимволы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Должно получиться вот такое число строк – 4389054. Как видите, 10 Гб у меня обработались примерно за полторы минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.34 s, sys: 256 ms, total: 2.6 s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python preprocess.py ../../data/stackoverflow.10kk.tsv ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389054 ../../data/stackoverflow.vw\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на обучающую, проверочную и тестовую части в равной пропорции - по  1463018 в каждый файл. Перемешивать не надо, первые 1463018 строк должны пойти в обучающую часть `stackoverflow_train.vw`, последние 1463018 – в тестовую `stackoverflow_test.vw`, оставшиеся – в проверочную `stackoverflow_valid.vw`. \n",
    "\n",
    "Также сохраните векторы ответов для проверочной и тестовой выборки в отдельные файлы `stackoverflow_valid_labels.txt` и `stackoverflow_test_labels.txt`.\n",
    "\n",
    "Тут вам помогут утилиты `head`, `tail`, `split`, `cat` и `cut`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "!head -1463018 ../../data/stackoverflow.vw > ../../data/stackoverflow_train.vw\n",
    "!sed -i '1,+1463017d' ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!wc -l ../../data/stackoverflow_train.vw\n",
    "!wc -l ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "!head -1463018 ../../data/stackoverflow.vw > ../../data/stackoverflow_valid.vw\n",
    "!sed -i '1,+1463017d' ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!wc -l ../../data/stackoverflow_valid.vw\n",
    "!wc -l ../../data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mv ../../data/stackoverflow.vw ../../data/stackoverflow_test.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!split -l 1463018 ../../data/stackoverflow.vw ../../data/stackoveflow_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21009852\r\n",
      "drwxrwxr-x 5 borowis borowis       12288 May  2 00:44 .\r\n",
      "drwxrwxr-x 7 borowis borowis        4096 Apr 21 23:47 ..\r\n",
      "-rw-rw-r-- 1 borowis borowis     3518607 Feb 28 20:27 adult.data.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     2096597 Mar 18 04:45 adult_test.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     3835686 Mar 18 04:45 adult_train.csv\r\n",
      "-rwxrwxr-x 1 borowis borowis     3361468 Apr 21 23:47 bank_train.csv\r\n",
      "-rwxrwxr-x 1 borowis borowis       55190 Apr 21 23:47 bank_train_target.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis       31107 Feb 28 20:27 beauty.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     1812928 Apr  2 22:40 credit_scoring_sample.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     3859157 Feb 28 20:27 credit_scoring_test.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     3948401 Feb 28 20:27 credit_scoring_train.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis         100 Feb 28 20:27 drugs-and-math.csv\r\n",
      "drwxrwxr-x 3 borowis borowis        4096 Apr 16 00:28 faces\r\n",
      "-rw-rw-r-- 1 borowis borowis       17063 Feb 28 20:27 girls.csv\r\n",
      "-rwxrwxr-x 1 borowis borowis        2873 Apr  2 22:40 hostel_factors.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis      813469 Mar 27 22:57 howpop_test.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    17861833 Mar 27 22:58 howpop_test.jsonlines.bz2\r\n",
      "-rw-rw-r-- 1 borowis borowis    34032627 Mar 27 22:59 howpop_train.csv\r\n",
      "drwxrwxrwx 4 borowis borowis        4096 Dec  9 17:38 imdb_reviews\r\n",
      "-rw-rw-r-- 1 borowis borowis    60878714 Apr 28 00:53 imdb_reviews.zip\r\n",
      "-rw-rw-r-- 1 borowis borowis        2233 Mar 21 18:26 microchip_tests.txt\r\n",
      "-rw-rw-r-- 1 borowis borowis      448768 Mar 18 04:45 mlcourse_open_first_survey_data.csv\r\n",
      "drwxrwxr-x 2 borowis borowis        4096 Apr 21 23:47 news_data\r\n",
      "-rw-rw-r-- 1 borowis borowis      239127 Mar 27 22:58 sample_submission.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis     1800682 Mar 27 16:54 site_dic.pkl\r\n",
      "-rw-rw-r-- 1 borowis borowis  1684710339 May  2 00:44 stackoveflow_segmentaa\r\n",
      "-rw-rw-r-- 1 borowis borowis  1683895638 May  2 00:44 stackoveflow_segmentab\r\n",
      "-rw-rw-r-- 1 borowis borowis  1689353139 May  2 00:44 stackoveflow_segmentac\r\n",
      "-rw-rw-r-- 1 borowis borowis 11085202682 May  1 18:25 stackoverflow.10kk.tsv\r\n",
      "-rw-rw-r-- 1 borowis borowis  5057959116 May  2 00:41 stackoverflow.vw\r\n",
      "-rwxrwxr-x 1 borowis borowis      279997 Feb 28 20:27 telecom_churn.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    19450729 Mar 27 16:55 test_sessions.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    60526801 Mar 27 16:57 train_sessions.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    11483103 Mar 27 16:55 train.zip\r\n",
      "-rwxrwxr-x 1 borowis borowis     1618040 Mar  9 17:56 video_games_sales.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    19450729 Mar 21 18:26 websites_test_sessions.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis    60526801 Mar 21 18:26 websites_train_sessions.csv\r\n",
      "-rw-rw-r-- 1 borowis borowis      583326 Apr 21 23:47 weights_heights.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463018 ../../data/stackoveflow_segmentaa\n",
      "1463018 ../../data/stackoveflow_segmentab\n",
      "1463018 ../../data/stackoveflow_segmentac\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data/stackoveflow_segmentaa\n",
    "!wc -l ../../data/stackoveflow_segmentab\n",
    "!wc -l ../../data/stackoveflow_segmentac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_out_labels = open(\"../../data/stackoveflow_segmentab_labels.txt\", 'w')\n",
    "with open(\"../../data/stackoveflow_segmentab\") as f:\n",
    "    for line in f:\n",
    "        splitted = line.split(\"|\")\n",
    "        \n",
    "        f_out_labels.write(str(splitted[0].strip()) + \"\\n\")\n",
    "        \n",
    "f_out_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463018 ../../data/stackoveflow_segmentab_labels.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data/stackoveflow_segmentab_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_out_labels = open(\"../../data/stackoveflow_segmentac_labels.txt\", 'w')\n",
    "with open(\"../../data/stackoveflow_segmentac\") as f:\n",
    "    for line in f:\n",
    "        splitted = line.split(\"|\")\n",
    "        \n",
    "        f_out_labels.write(str(splitted[0].strip()) + \"\\n\")\n",
    "        \n",
    "f_out_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463018 ../../data/stackoveflow_segmentac_labels.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../../data/stackoveflow_segmentac_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Обучение и проверка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Vowpal Wabbit на выборке `stackoverflow_train.vw` 9 раз, перебирая параметры passes (1,3,5), ngram (1,2,3).\n",
    "Остальные параметры укажите следующие: bit_precision=28 и seed=17. Также скажите VW, что это 10-классовая задача.\n",
    "\n",
    "Проверяйте долю правильных ответов на выборке `stackoverflow_valid.vw`. Выберите лучшую модель и проверьте качество на выборке `stackoverflow_test.vw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_1_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.812500 0.875000           16           16.0        7        7      209\n",
      "0.812500 0.812500           32           32.0        7        5      174\n",
      "0.781250 0.750000           64           64.0        3        3      204\n",
      "0.671875 0.562500          128          128.0        1        5       29\n",
      "0.609375 0.546875          256          256.0        5        5      169\n",
      "0.541016 0.472656          512          512.0        2        2      303\n",
      "0.451172 0.361328         1024         1024.0        3        3      123\n",
      "0.375977 0.300781         2048         2048.0        1        5       83\n",
      "0.311523 0.247070         4096         4096.0        1        1       79\n",
      "0.269287 0.227051         8192         8192.0        2        2      112\n",
      "0.226868 0.184448        16384        16384.0        7        7      252\n",
      "0.188995 0.151123        32768        32768.0        4        5      134\n",
      "0.162964 0.136932        65536        65536.0        5        5      145\n",
      "0.141556 0.120148       131072       131072.0        7        2      255\n",
      "0.123825 0.106094       262144       262144.0        7        7      101\n",
      "0.112268 0.100712       524288       524288.0        1        1      818\n",
      "0.103132 0.093996      1048576      1048576.0        1        1      571\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.098885\n",
      "total feature number = 291954690\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_1_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.765625 0.750000           64           64.0        3        3      406\n",
      "0.664062 0.562500          128          128.0        1        7       56\n",
      "0.597656 0.531250          256          256.0        5        3      336\n",
      "0.527344 0.457031          512          512.0        2        2      604\n",
      "0.429688 0.332031         1024         1024.0        3        3      244\n",
      "0.362793 0.295898         2048         2048.0        1        5      164\n",
      "0.298096 0.233398         4096         4096.0        1        1      156\n",
      "0.249878 0.201660         8192         8192.0        2        2      222\n",
      "0.210266 0.170654        16384        16384.0        7        7      502\n",
      "0.175262 0.140259        32768        32768.0        4        5      266\n",
      "0.148163 0.121063        65536        65536.0        5        5      288\n",
      "0.127380 0.106598       131072       131072.0        7        2      508\n",
      "0.109459 0.091537       262144       262144.0        7        7      200\n",
      "0.097469 0.085480       524288       524288.0        1        1     1634\n",
      "0.087223 0.076977      1048576      1048576.0        1        1     1140\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083143\n",
      "total feature number = 580983344\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_1_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        7        7      622\n",
      "0.781250 0.812500           32           32.0        7        2      517\n",
      "0.765625 0.750000           64           64.0        3        3      607\n",
      "0.656250 0.546875          128          128.0        1        7       82\n",
      "0.605469 0.554688          256          256.0        5        1      502\n",
      "0.535156 0.464844          512          512.0        2        2      904\n",
      "0.442383 0.349609         1024         1024.0        3        3      364\n",
      "0.380371 0.318359         2048         2048.0        1        5      244\n",
      "0.317383 0.254395         4096         4096.0        1        1      232\n",
      "0.266724 0.216064         8192         8192.0        2        2      331\n",
      "0.222290 0.177856        16384        16384.0        7        7      751\n",
      "0.184631 0.146973        32768        32768.0        4        5      397\n",
      "0.155502 0.126373        65536        65536.0        5        5      430\n",
      "0.132919 0.110336       131072       131072.0        7        7      760\n",
      "0.113468 0.094017       262144       262144.0        7        7      298\n",
      "0.101650 0.089832       524288       524288.0        1        1     2449\n",
      "0.090106 0.078562      1048576      1048576.0        1        1     1708\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.086319\n",
      "total feature number = 868548985\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_3_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_3_1_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.781250 0.812500           32           32.0        1        7      404\n",
      "0.718750 0.656250           64           64.0        7        7      103\n",
      "0.664062 0.609375          128          128.0        5        5      276\n",
      "0.625000 0.585938          256          256.0        1        1      102\n",
      "0.546875 0.468750          512          512.0        2        5       68\n",
      "0.450195 0.353516         1024         1024.0        1        1      132\n",
      "0.387207 0.324219         2048         2048.0        7        7       71\n",
      "0.318115 0.249023         4096         4096.0        2        2      319\n",
      "0.267578 0.217041         8192         8192.0        5        2       24\n",
      "0.227661 0.187744        16384        16384.0        3        3      581\n",
      "0.190674 0.153687        32768        32768.0        3        3       28\n",
      "0.164215 0.137756        65536        65536.0        4        4      184\n",
      "0.142067 0.119919       131072       131072.0        2        2       95\n",
      "0.125511 0.108955       262144       262144.0        5        5      232\n",
      "0.112450 0.099388       524288       524288.0        6        6      142\n",
      "0.103465 0.094481      1048576      1048576.0        1        1      422\n",
      "0.096127 0.096127      2097152      2097152.0        5        5      696 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087949 h\n",
      "total feature number = 788274150\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_3_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_3_2_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.687500 0.625000           16           16.0        2        7      316\n",
      "0.687500 0.687500           32           32.0        1        7      806\n",
      "0.687500 0.687500           64           64.0        7        7      204\n",
      "0.601562 0.515625          128          128.0        5        5      550\n",
      "0.574219 0.546875          256          256.0        1        1      202\n",
      "0.503906 0.433594          512          512.0        2        5      134\n",
      "0.432617 0.361328         1024         1024.0        1        1      262\n",
      "0.370605 0.308594         2048         2048.0        7        7      140\n",
      "0.299072 0.227539         4096         4096.0        2        2      636\n",
      "0.252197 0.205322         8192         8192.0        5        2       46\n",
      "0.215515 0.178833        16384        16384.0        3        3     1160\n",
      "0.178619 0.141724        32768        32768.0        3        3       54\n",
      "0.149918 0.121216        65536        65536.0        4        4      366\n",
      "0.128204 0.106491       131072       131072.0        2        2      188\n",
      "0.111256 0.094307       262144       262144.0        5        5      462\n",
      "0.097771 0.084286       524288       524288.0        6        6      282\n",
      "0.087827 0.077883      1048576      1048576.0        1        1      842\n",
      "0.079544 0.079544      2097152      2097152.0        5        5     1390 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071968 h\n",
      "total feature number = 1568647998\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_3_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_3_3_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.750000 0.750000           32           32.0        1        7     1207\n",
      "0.718750 0.687500           64           64.0        7        7      304\n",
      "0.640625 0.562500          128          128.0        5        5      823\n",
      "0.609375 0.578125          256          256.0        1        1      301\n",
      "0.521484 0.433594          512          512.0        2        5      199\n",
      "0.442383 0.363281         1024         1024.0        1        1      391\n",
      "0.378906 0.315430         2048         2048.0        7        7      208\n",
      "0.310791 0.242676         4096         4096.0        2        2      952\n",
      "0.265747 0.220703         8192         8192.0        5        2       67\n",
      "0.223755 0.181763        16384        16384.0        3        3     1738\n",
      "0.186035 0.148315        32768        32768.0        3        3       79\n",
      "0.155670 0.125305        65536        65536.0        4        4      547\n",
      "0.133133 0.110596       131072       131072.0        2        2      280\n",
      "0.115284 0.097435       262144       262144.0        5        5      691\n",
      "0.101097 0.086910       524288       524288.0        6        6      421\n",
      "0.090279 0.079460      1048576      1048576.0        1        1     1261\n",
      "0.081248 0.081248      2097152      2097152.0        5        5     2083 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074053 h\n",
      "total feature number = 2345071710\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_5_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_5_1_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.781250 0.812500           32           32.0        1        7      404\n",
      "0.718750 0.656250           64           64.0        7        7      103\n",
      "0.664062 0.609375          128          128.0        5        5      276\n",
      "0.625000 0.585938          256          256.0        1        1      102\n",
      "0.546875 0.468750          512          512.0        2        5       68\n",
      "0.450195 0.353516         1024         1024.0        1        1      132\n",
      "0.387207 0.324219         2048         2048.0        7        7       71\n",
      "0.318115 0.249023         4096         4096.0        2        2      319\n",
      "0.267578 0.217041         8192         8192.0        5        2       24\n",
      "0.227661 0.187744        16384        16384.0        3        3      581\n",
      "0.190674 0.153687        32768        32768.0        3        3       28\n",
      "0.164215 0.137756        65536        65536.0        4        4      184\n",
      "0.142067 0.119919       131072       131072.0        2        2       95\n",
      "0.125511 0.108955       262144       262144.0        5        5      232\n",
      "0.112450 0.099388       524288       524288.0        6        6      142\n",
      "0.103465 0.094481      1048576      1048576.0        1        1      422\n",
      "0.096127 0.096127      2097152      2097152.0        5        5      696 h\n",
      "0.092037 0.087947      4194304      4194304.0        1        1      216 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.087949 h\n",
      "total feature number = 1313790250\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_5_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_5_2_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.687500 0.625000           16           16.0        2        7      316\n",
      "0.687500 0.687500           32           32.0        1        7      806\n",
      "0.687500 0.687500           64           64.0        7        7      204\n",
      "0.601562 0.515625          128          128.0        5        5      550\n",
      "0.574219 0.546875          256          256.0        1        1      202\n",
      "0.503906 0.433594          512          512.0        2        5      134\n",
      "0.432617 0.361328         1024         1024.0        1        1      262\n",
      "0.370605 0.308594         2048         2048.0        7        7      140\n",
      "0.299072 0.227539         4096         4096.0        2        2      636\n",
      "0.252197 0.205322         8192         8192.0        5        2       46\n",
      "0.215515 0.178833        16384        16384.0        3        3     1160\n",
      "0.178619 0.141724        32768        32768.0        3        3       54\n",
      "0.149918 0.121216        65536        65536.0        4        4      366\n",
      "0.128204 0.106491       131072       131072.0        2        2      188\n",
      "0.111256 0.094307       262144       262144.0        5        5      462\n",
      "0.097771 0.084286       524288       524288.0        6        6      282\n",
      "0.087827 0.077883      1048576      1048576.0        1        1      842\n",
      "0.079544 0.079544      2097152      2097152.0        5        5     1390 h\n",
      "0.075875 0.072205      4194304      4194304.0        1        1      430 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071968 h\n",
      "total feature number = 2614413330\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_5_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/stackoverflow_5_3_cache\n",
      "Warning: you tried to make two write caches.  Only the first one will be made.\n",
      "Reading datafile = ../../data/stackoveflow_segmentaa\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.750000 0.750000           32           32.0        1        7     1207\n",
      "0.718750 0.687500           64           64.0        7        7      304\n",
      "0.640625 0.562500          128          128.0        5        5      823\n",
      "0.609375 0.578125          256          256.0        1        1      301\n",
      "0.521484 0.433594          512          512.0        2        5      199\n",
      "0.442383 0.363281         1024         1024.0        1        1      391\n",
      "0.378906 0.315430         2048         2048.0        7        7      208\n",
      "0.310791 0.242676         4096         4096.0        2        2      952\n",
      "0.265747 0.220703         8192         8192.0        5        2       67\n",
      "0.223755 0.181763        16384        16384.0        3        3     1738\n",
      "0.186035 0.148315        32768        32768.0        3        3       79\n",
      "0.155670 0.125305        65536        65536.0        4        4      547\n",
      "0.133133 0.110596       131072       131072.0        2        2      280\n",
      "0.115284 0.097435       262144       262144.0        5        5      691\n",
      "0.101097 0.086910       524288       524288.0        6        6      421\n",
      "0.090279 0.079460      1048576      1048576.0        1        1     1261\n",
      "0.081248 0.081248      2097152      2097152.0        5        5     2083 h\n",
      "0.077922 0.074596      4194304      4194304.0        1        1      643 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.074053 h\n",
      "total feature number = 3908452850\n",
      "CPU times: user 34.7 s, sys: 3.19 s, total: 37.9 s\n",
      "Wall time: 32min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "!vw --oaa 10 --passes 1 --ngram 1 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_1_1.vw\n",
    "!vw --oaa 10 --passes 1 --ngram 2 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_1_2.vw\n",
    "!vw --oaa 10 --passes 1 --ngram 3 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_1_3.vw\n",
    "\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_3_1_cache --passes 3 --ngram 1 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_3_1.vw\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_3_2_cache --passes 3 --ngram 2 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_3_2.vw\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_3_3_cache --passes 3 --ngram 3 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_3_3.vw\n",
    "\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_5_1_cache --passes 5 --ngram 1 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_5_1.vw\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_5_2_cache --passes 5 --ngram 2 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_5_2.vw\n",
    "!vw --oaa 10 -c --cache_file ../../data/stackoverflow_5_3_cache --passes 5 --ngram 3 -d ../../data/stackoveflow_segmentaa -b 28 --random_seed 17 -f ../../data/stackoverflow_model_5_3.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_1_1.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      178\n",
      "0.000000 0.000000            2            2.0        7        7       74\n",
      "0.000000 0.000000            4            4.0        5        5      259\n",
      "0.000000 0.000000            8            8.0        7        7      144\n",
      "0.000000 0.000000           16           16.0        6        6      359\n",
      "0.000000 0.000000           32           32.0        2        2      400\n",
      "0.062500 0.125000           64           64.0        5        5     1061\n",
      "0.070312 0.078125          128          128.0        2        2      132\n",
      "0.085938 0.101562          256          256.0        6        6       88\n",
      "0.078125 0.070312          512          512.0        7        6       35\n",
      "0.073242 0.068359         1024         1024.0        1        1       88\n",
      "0.084473 0.095703         2048         2048.0        2        2      243\n",
      "0.083008 0.081543         4096         4096.0        2        2      263\n",
      "0.085205 0.087402         8192         8192.0        7        7      272\n",
      "0.085022 0.084839        16384        16384.0        2        2      127\n",
      "0.083527 0.082031        32768        32768.0        6        6       91\n",
      "0.084702 0.085876        65536        65536.0        5        5      286\n",
      "0.084320 0.083939       131072       131072.0        5        5      176\n",
      "0.084423 0.084526       262144       262144.0        2        2      248\n",
      "0.084423 0.084423       524288       524288.0        7        7       36\n",
      "0.084977 0.085531      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.084846\n",
      "total feature number = 291768232\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_1_2.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.031250          128          128.0        2        2      262\n",
      "0.074219 0.101562          256          256.0        6        6      174\n",
      "0.058594 0.042969          512          512.0        7        7       68\n",
      "0.057617 0.056641         1024         1024.0        1        1      174\n",
      "0.062500 0.067383         2048         2048.0        2        2      484\n",
      "0.063965 0.065430         4096         4096.0        2        2      524\n",
      "0.066528 0.069092         8192         8192.0        7        7      542\n",
      "0.069092 0.071655        16384        16384.0        2        2      252\n",
      "0.068390 0.067688        32768        32768.0        6        6      180\n",
      "0.069061 0.069733        65536        65536.0        5        5      570\n",
      "0.068672 0.068283       131072       131072.0        5        5      350\n",
      "0.068424 0.068176       262144       262144.0        2        2      494\n",
      "0.068739 0.069054       524288       524288.0        7        7       70\n",
      "0.068953 0.069166      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.068916\n",
      "total feature number = 580610428\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_1_3.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.062500 0.000000           16           16.0        6        6     1072\n",
      "0.031250 0.000000           32           32.0        2        2     1195\n",
      "0.046875 0.062500           64           64.0        5        5     3178\n",
      "0.046875 0.046875          128          128.0        2        2      391\n",
      "0.078125 0.109375          256          256.0        6        6      259\n",
      "0.066406 0.054688          512          512.0        7        7      100\n",
      "0.065430 0.064453         1024         1024.0        1        1      259\n",
      "0.066406 0.067383         2048         2048.0        2        2      724\n",
      "0.066895 0.067383         4096         4096.0        2        2      784\n",
      "0.069946 0.072998         8192         8192.0        7        7      811\n",
      "0.072388 0.074829        16384        16384.0        2        2      376\n",
      "0.071899 0.071411        32768        32768.0        6        6      268\n",
      "0.072311 0.072723        65536        65536.0        5        5      853\n",
      "0.071831 0.071350       131072       131072.0        5        5      523\n",
      "0.070805 0.069778       262144       262144.0        2        2      739\n",
      "0.071125 0.071445       524288       524288.0        7        7      103\n",
      "0.071424 0.071722      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071397\n",
      "total feature number = 867989607\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_3_1.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      178\n",
      "0.000000 0.000000            2            2.0        7        7       74\n",
      "0.000000 0.000000            4            4.0        5        5      259\n",
      "0.000000 0.000000            8            8.0        7        7      144\n",
      "0.000000 0.000000           16           16.0        6        6      359\n",
      "0.000000 0.000000           32           32.0        2        2      400\n",
      "0.062500 0.125000           64           64.0        5        5     1061\n",
      "0.054688 0.046875          128          128.0        2        2      132\n",
      "0.070312 0.085938          256          256.0        6        6       88\n",
      "0.068359 0.066406          512          512.0        7        6       35\n",
      "0.073242 0.078125         1024         1024.0        1        1       88\n",
      "0.081055 0.088867         2048         2048.0        2        2      243\n",
      "0.087158 0.093262         4096         4096.0        2        2      263\n",
      "0.085693 0.084229         8192         8192.0        7        7      272\n",
      "0.084412 0.083130        16384        16384.0        2        2      127\n",
      "0.083984 0.083557        32768        32768.0        6        6       91\n",
      "0.085846 0.087708        65536        65536.0        5        5      286\n",
      "0.085564 0.085281       131072       131072.0        5        5      176\n",
      "0.085381 0.085197       262144       262144.0        2        2      248\n",
      "0.085276 0.085171       524288       524288.0        7        7       36\n",
      "0.085725 0.086174      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.085631\n",
      "total feature number = 291768232\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_3_2.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.031250          128          128.0        2        2      262\n",
      "0.074219 0.101562          256          256.0        6        6      174\n",
      "0.062500 0.050781          512          512.0        7        7       68\n",
      "0.060547 0.058594         1024         1024.0        1        1      174\n",
      "0.064453 0.068359         2048         2048.0        2        2      484\n",
      "0.064453 0.064453         4096         4096.0        2        2      524\n",
      "0.069580 0.074707         8192         8192.0        7        7      542\n",
      "0.071167 0.072754        16384        16384.0        2        2      252\n",
      "0.071045 0.070923        32768        32768.0        6        2      180\n",
      "0.072067 0.073090        65536        65536.0        5        5      570\n",
      "0.071945 0.071823       131072       131072.0        5        5      350\n",
      "0.071735 0.071526       262144       262144.0        2        2      494\n",
      "0.072044 0.072353       524288       524288.0        7        7       70\n",
      "0.072302 0.072559      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.072199\n",
      "total feature number = 580610428\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_3_3.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.125000 0.125000           16           16.0        6        6     1072\n",
      "0.062500 0.000000           32           32.0        2        2     1195\n",
      "0.062500 0.062500           64           64.0        5        5     3178\n",
      "0.054688 0.046875          128          128.0        2        2      391\n",
      "0.074219 0.093750          256          256.0        6        6      259\n",
      "0.064453 0.054688          512          512.0        7        7      100\n",
      "0.061523 0.058594         1024         1024.0        1        1      259\n",
      "0.069336 0.077148         2048         2048.0        2        2      724\n",
      "0.069824 0.070312         4096         4096.0        2        2      784\n",
      "0.071167 0.072510         8192         8192.0        7        7      811\n",
      "0.073853 0.076538        16384        16384.0        2        2      376\n",
      "0.073303 0.072754        32768        32768.0        6        2      268\n",
      "0.074036 0.074768        65536        65536.0        5        5      853\n",
      "0.073822 0.073608       131072       131072.0        5        5      523\n",
      "0.073242 0.072662       262144       262144.0        2        2      739\n",
      "0.073513 0.073784       524288       524288.0        7        7      103\n",
      "0.073659 0.073805      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073612\n",
      "total feature number = 867989607\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_5_1.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      178\n",
      "0.000000 0.000000            2            2.0        7        7       74\n",
      "0.000000 0.000000            4            4.0        5        5      259\n",
      "0.000000 0.000000            8            8.0        7        7      144\n",
      "0.000000 0.000000           16           16.0        6        6      359\n",
      "0.000000 0.000000           32           32.0        2        2      400\n",
      "0.062500 0.125000           64           64.0        5        5     1061\n",
      "0.062500 0.062500          128          128.0        2        2      132\n",
      "0.082031 0.101562          256          256.0        6        6       88\n",
      "0.078125 0.074219          512          512.0        7        6       35\n",
      "0.074219 0.070312         1024         1024.0        1        1       88\n",
      "0.082520 0.090820         2048         2048.0        2        2      243\n",
      "0.088135 0.093750         4096         4096.0        2        2      263\n",
      "0.086060 0.083984         8192         8192.0        7        7      272\n",
      "0.085876 0.085693        16384        16384.0        2        2      127\n",
      "0.085205 0.084534        32768        32768.0        6        6       91\n",
      "0.086761 0.088318        65536        65536.0        5        5      286\n",
      "0.086632 0.086502       131072       131072.0        5        5      176\n",
      "0.086311 0.085991       262144       262144.0        2        2      248\n",
      "0.086119 0.085926       524288       524288.0        7        7       36\n",
      "0.086538 0.086958      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.086346\n",
      "total feature number = 291768232\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_5_2.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.078125 0.093750           64           64.0        5        5     2120\n",
      "0.054688 0.031250          128          128.0        2        2      262\n",
      "0.078125 0.101562          256          256.0        6        6      174\n",
      "0.066406 0.054688          512          512.0        7        7       68\n",
      "0.059570 0.052734         1024         1024.0        1        1      174\n",
      "0.063965 0.068359         2048         2048.0        2        2      484\n",
      "0.064941 0.065918         4096         4096.0        2        2      524\n",
      "0.068604 0.072266         8192         8192.0        7        7      542\n",
      "0.070740 0.072876        16384        16384.0        2        2      252\n",
      "0.070587 0.070435        32768        32768.0        6        6      180\n",
      "0.071198 0.071808        65536        65536.0        5        5      570\n",
      "0.070648 0.070099       131072       131072.0        5        5      350\n",
      "0.070221 0.069794       262144       262144.0        2        2      494\n",
      "0.070614 0.071007       524288       524288.0        7        7       70\n",
      "0.070823 0.071032      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.070765\n",
      "total feature number = 580610428\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_5_3.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.125000 0.125000           16           16.0        6        6     1072\n",
      "0.062500 0.000000           32           32.0        2        2     1195\n",
      "0.078125 0.093750           64           64.0        5        5     3178\n",
      "0.070312 0.062500          128          128.0        2        2      391\n",
      "0.082031 0.093750          256          256.0        6        6      259\n",
      "0.066406 0.050781          512          512.0        7        7      100\n",
      "0.062500 0.058594         1024         1024.0        1        1      259\n",
      "0.068848 0.075195         2048         2048.0        2        2      724\n",
      "0.068604 0.068359         4096         4096.0        2        2      784\n",
      "0.071411 0.074219         8192         8192.0        7        7      811\n",
      "0.074280 0.077148        16384        16384.0        2        2      376\n",
      "0.074005 0.073730        32768        32768.0        6        2      268\n",
      "0.073898 0.073792        65536        65536.0        5        5      853\n",
      "0.073792 0.073685       131072       131072.0        5        5      523\n",
      "0.073288 0.072784       262144       262144.0        2        2      739\n",
      "0.073629 0.073971       524288       524288.0        7        7      103\n",
      "0.073862 0.074095      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073833\n",
      "total feature number = 867989607\n",
      "CPU times: user 7.48 s, sys: 568 ms, total: 8.05 s\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -t -i ../../data/stackoverflow_model_1_1.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_1_1.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_1_2.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_1_2.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_1_3.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_1_3.predictions --random_seed 17 \n",
    "\n",
    "!vw -t -i ../../data/stackoverflow_model_3_1.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_3_1.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_3_2.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_3_2.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_3_3.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_3_3.predictions --random_seed 17 \n",
    "\n",
    "!vw -t -i ../../data/stackoverflow_model_5_1.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_5_1.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_5_2.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_5_2.predictions --random_seed 17 \n",
    "!vw -t -i ../../data/stackoverflow_model_5_3.vw -d ../../data/stackoveflow_segmentab -p ../../data/stackoverflow_model_5_3.predictions --random_seed 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow_model_1_1.predictions: 0.9151541539475249\n",
      "stackoverflow_model_1_2.predictions: 0.9310835546794366\n",
      "stackoverflow_model_1_3.predictions: 0.9286030657175783\n",
      "stackoverflow_model_3_1.predictions: 0.9143694746066009\n",
      "stackoverflow_model_3_2.predictions: 0.9278012984119129\n",
      "stackoverflow_model_3_3.predictions: 0.9263877819685062\n",
      "stackoverflow_model_5_1.predictions: 0.9136538306432320\n",
      "stackoverflow_model_5_2.predictions: 0.9292353204130093\n",
      "stackoverflow_model_5_3.predictions: 0.9261670054640476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_labels = np.loadtxt('../../data/stackoveflow_segmentab_labels.txt')\n",
    "model_predictions = ['stackoverflow_model_1_1.predictions', 'stackoverflow_model_1_2.predictions', 'stackoverflow_model_1_3.predictions', \\\n",
    "                     'stackoverflow_model_3_1.predictions', 'stackoverflow_model_3_2.predictions', 'stackoverflow_model_3_3.predictions', \\\n",
    "                     'stackoverflow_model_5_1.predictions', 'stackoverflow_model_5_2.predictions', 'stackoverflow_model_5_3.predictions']\n",
    "for model in model_predictions:\n",
    "    vw_pred = np.loadtxt('../../data/' + model)\n",
    "    print (model + ': ' + \"{:2.16f}\".format(accuracy_score(test_labels, vw_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 1.</font> Какое сочетание параметров дает наибольшую долю правильных ответов на проверочной выборке `stackoverflow_valid.vw`?\n",
    "- Биграммы и 3 прохода по выборке\n",
    "- Триграммы и 5 проходов по выборке\n",
    "- ** Биграммы и 1 проход по выборке **\n",
    "- Униграммы и 1 проход по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте лучшую (по доле правильных ответов на валидации) модель на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_1_2_test.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentac\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        9        9      370\n",
      "0.000000 0.000000            2            2.0        1        1       62\n",
      "0.000000 0.000000            4            4.0        5        5      158\n",
      "0.000000 0.000000            8            8.0        2        2      116\n",
      "0.000000 0.000000           16           16.0        6        6       74\n",
      "0.031250 0.062500           32           32.0        1        1      324\n",
      "0.046875 0.062500           64           64.0        4        4       52\n",
      "0.054688 0.062500          128          128.0        7        7      250\n",
      "0.066406 0.078125          256          256.0        7        1      192\n",
      "0.072266 0.078125          512          512.0        1        1       70\n",
      "0.064453 0.056641         1024         1024.0        2        2      176\n",
      "0.070801 0.077148         2048         2048.0        2        2       70\n",
      "0.070801 0.070801         4096         4096.0        3        3      784\n",
      "0.071533 0.072266         8192         8192.0        4        4      312\n",
      "0.070557 0.069580        16384        16384.0        4        4      296\n",
      "0.070068 0.069580        32768        32768.0        1        1      490\n",
      "0.070465 0.070862        65536        65536.0        6        6      488\n",
      "0.069366 0.068268       131072       131072.0       10       10      540\n",
      "0.068981 0.068596       262144       262144.0        3        3     1316\n",
      "0.069118 0.069256       524288       524288.0        7        7      516\n",
      "0.069042 0.068966      1048576      1048576.0        7        6     1150\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.068809\n",
      "total feature number = 582313634\n",
      "CPU times: user 836 ms, sys: 60 ms, total: 896 ms\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "!vw -t -i ../../data/stackoverflow_model_1_2.vw -d ../../data/stackoveflow_segmentac -p ../../data/stackoverflow_model_1_2_test.predictions --random_seed 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931190867098\n"
     ]
    }
   ],
   "source": [
    "test_labels = np.loadtxt('../../data/stackoveflow_segmentac_labels.txt')\n",
    "vw_pred = np.loadtxt('../../data/stackoverflow_model_1_2_test.predictions')\n",
    "print (accuracy_score(test_labels, vw_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9310835546794366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 2.</font> Как соотносятся доли правильных ответов лучшей (по доле правильных ответов на валидации) модели на проверочной и на тестовой выборках? (здесь % – это процентный пункт, т.е., скажем, снижение с 50% до 40% – это на 10%, а не 20%).\n",
    "- На тестовой ниже примерно на 2%\n",
    "- На тестовой ниже примерно на 3%\n",
    "- ** Результаты почти одинаковы – отличаются меньше чем на 0.5% **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите VW с параметрами, подобранными на проверочной выборке, теперь на объединении обучающей и проверочной выборок. Посчитайте долю правильных ответов на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2926036 ../../data/stackoveflow_segmentaab\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = ../../data/stackoverflow_model_full.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentaab\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.765625 0.750000           64           64.0        3        3      406\n",
      "0.664062 0.562500          128          128.0        1        7       56\n",
      "0.597656 0.531250          256          256.0        5        3      336\n",
      "0.527344 0.457031          512          512.0        2        2      604\n",
      "0.429688 0.332031         1024         1024.0        3        3      244\n",
      "0.362793 0.295898         2048         2048.0        1        5      164\n",
      "0.298096 0.233398         4096         4096.0        1        1      156\n",
      "0.249878 0.201660         8192         8192.0        2        2      222\n",
      "0.210266 0.170654        16384        16384.0        7        7      502\n",
      "0.175262 0.140259        32768        32768.0        4        5      266\n",
      "0.148163 0.121063        65536        65536.0        5        5      288\n",
      "0.127380 0.106598       131072       131072.0        7        2      508\n",
      "0.109459 0.091537       262144       262144.0        7        7      200\n",
      "0.097469 0.085480       524288       524288.0        1        1     1634\n",
      "0.087223 0.076977      1048576      1048576.0        1        1     1140\n",
      "0.078517 0.069811      2097152      2097152.0        2        2      342\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 2926036\n",
      "passes used = 1\n",
      "weighted example sum = 2926036.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.075154\n",
      "total feature number = 1161593772\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = ../../data/stackoverflow_model_full_test.predictions\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = ../../data/stackoveflow_segmentac\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        9        9      370\n",
      "0.000000 0.000000            2            2.0        1        1       62\n",
      "0.000000 0.000000            4            4.0        5        5      158\n",
      "0.000000 0.000000            8            8.0        2        2      116\n",
      "0.000000 0.000000           16           16.0        6        6       74\n",
      "0.031250 0.062500           32           32.0        1        1      324\n",
      "0.062500 0.093750           64           64.0        4        2       52\n",
      "0.062500 0.062500          128          128.0        7        7      250\n",
      "0.066406 0.070312          256          256.0        7        1      192\n",
      "0.072266 0.078125          512          512.0        1        1       70\n",
      "0.067383 0.062500         1024         1024.0        2        2      176\n",
      "0.069336 0.071289         2048         2048.0        2        2       70\n",
      "0.067383 0.065430         4096         4096.0        3        3      784\n",
      "0.066406 0.065430         8192         8192.0        4        4      312\n",
      "0.065857 0.065308        16384        16384.0        4        4      296\n",
      "0.066010 0.066162        32768        32768.0        1        1      490\n",
      "0.066345 0.066681        65536        65536.0        6        6      488\n",
      "0.065483 0.064621       131072       131072.0       10       10      540\n",
      "0.065147 0.064812       262144       262144.0        3        3     1316\n",
      "0.065342 0.065536       524288       524288.0        7        7      516\n",
      "0.065061 0.064779      1048576      1048576.0        7        1     1150\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1463018\n",
      "passes used = 1\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.064813\n",
      "total feature number = 582313634\n"
     ]
    }
   ],
   "source": [
    "''' ВАШ КОД ЗДЕСЬ '''\n",
    "!cat ../../data/stackoveflow_segmentaa ../../data/stackoveflow_segmentab > ../../data/stackoveflow_segmentaab\n",
    "!wc -l ../../data/stackoveflow_segmentaab\n",
    "!vw --oaa 10 --passes 1 --ngram 2 -d ../../data/stackoveflow_segmentaab -b 28 --random_seed 17 -f ../../data/stackoverflow_model_full.vw\n",
    "!vw -t -i ../../data/stackoverflow_model_full.vw -d ../../data/stackoveflow_segmentac -p ../../data/stackoverflow_model_full_test.predictions --random_seed 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935186716773\n"
     ]
    }
   ],
   "source": [
    "test_labels = np.loadtxt('../../data/stackoveflow_segmentac_labels.txt')\n",
    "vw_pred = np.loadtxt('../../data/stackoverflow_model_full_test.predictions')\n",
    "print (accuracy_score(test_labels, vw_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9310835546794366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 3.</font> На сколько процентных пунктов повысилась доля правильных ответов модели после обучения на вдвое большей выборке (обучающая `stackoverflow_train.vw` + проверочная `stackoverflow_valid.vw`) по сравнению с моделью, обученной только на `stackoverflow_train.vw`?\n",
    " - 0.1%\n",
    " - ** 0.4% **\n",
    " - 0.8%\n",
    " - 1.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы только познакомились с Vowpal Wabbit. Что еще можно попробовать:\n",
    " - Классификация с несколькими ответами (multilabel classification, аргумент  `multilabel_oaa`) – формат данных тут как раз под такую задачу\n",
    " - Настройка параметров VW с hyperopt, авторы библиотеки утверждают, что качество должно сильно зависеть от параметров изменения шага градиентного спуска (`initial_t` и `power_t`). Также можно потестировать разные функции потерь – обучать логистическую регресиию или линейный SVM\n",
    " - Познакомиться с факторизационными машинами и их реализацией в VW (аргумент `lrq`)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
